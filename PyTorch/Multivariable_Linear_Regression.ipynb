{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1c8f7de9350>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariable Linear Regression(다중 선형 회귀)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Simple Linear Regression(단순 선형 회귀)` : 미지수 $x$가 1개인 선형 회귀\n",
    "- `Multivariable Linear Regression(다중 선형 회귀)` : 미지수 $x$가 2개 이상인 선형 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "독립 변수 $x$의 개수가 3개인 데이터가 있을때 결과를 예측하는 모델\n",
    "\n",
    "독립 변수 $x$의 개수가 3개인 일차 방정식을 구하면\n",
    "\n",
    "$ H(x) = w_1x_1 + w_2x_2 + w_3x_3 + b $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PyTorch로 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터 \n",
    "x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]])\n",
    "x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])\n",
    "x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 w와 편향 b 초기화\n",
    "w1 = torch.zeros(1, requires_grad=True)\n",
    "w2 = torch.zeros(1, requires_grad=True)\n",
    "w3 = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000                w1: 0.294                w2: 0.294                w3: 0.297                b: 0.003                Cost: 29661.800781\n",
      "Epoch  100/1000                w1: 0.674                w2: 0.661                w3: 0.676                b: 0.008                Cost: 1.563634\n",
      "Epoch  200/1000                w1: 0.679                w2: 0.655                w3: 0.677                b: 0.008                Cost: 1.497607\n",
      "Epoch  300/1000                w1: 0.684                w2: 0.649                w3: 0.677                b: 0.008                Cost: 1.435026\n",
      "Epoch  400/1000                w1: 0.689                w2: 0.643                w3: 0.678                b: 0.008                Cost: 1.375730\n",
      "Epoch  500/1000                w1: 0.694                w2: 0.638                w3: 0.678                b: 0.009                Cost: 1.319511\n",
      "Epoch  600/1000                w1: 0.699                w2: 0.633                w3: 0.679                b: 0.009                Cost: 1.266222\n",
      "Epoch  700/1000                w1: 0.704                w2: 0.627                w3: 0.679                b: 0.009                Cost: 1.215696\n",
      "Epoch  800/1000                w1: 0.709                w2: 0.622                w3: 0.679                b: 0.009                Cost: 1.167818\n",
      "Epoch  900/1000                w1: 0.713                w2: 0.617                w3: 0.680                b: 0.009                Cost: 1.122429\n",
      "Epoch 1000/1000                w1: 0.718                w2: 0.613                w3: 0.680                b: 0.009                Cost: 1.079378\n"
     ]
    }
   ],
   "source": [
    "# 가설, 비용 함수, 옵티마이저를 선언한 후에 경사 하강법을 사용\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([w1, w2, w3, b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b\n",
    "\n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch:4d}/{nb_epochs} \\\n",
    "               w1: {w1.item():.3f} \\\n",
    "               w2: {w2.item():.3f} \\\n",
    "               w3: {w3.item():.3f} \\\n",
    "               b: {b.item():.3f} \\\n",
    "               Cost: {cost.item():.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 행렬 연산 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력이나 가중치의 개수가 늘어나면 일일히 선언에 어려움이 있음\n",
    "\n",
    "행렬 곱셈 연산(또는 벡터의 내적)을 사용해 개선 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ H(X) = w_1x_1 + w_2x_2 + w_3x_3 $\n",
    "\n",
    "이 식을 행렬(벡터)의 연산으로 표현 하면 아래와 같음\n",
    "\n",
    "$ \\begin{pmatrix} x_1&&x_2&&x_3 \\end{pmatrix} \\cdot \\begin{pmatrix} w_1\\\\w_2\\\\w_3 \\end{pmatrix} = (x_1w_1 + x_2w_2 + x_3w_3) $\n",
    "\n",
    "두 벡터를 각각 $X$와 $W$로 표현한다면 다음과 같음\n",
    "\n",
    "$ H(X) = XW $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `sample` : 전체 훈련 데이터의 개수를 셀 수 있는 1개의 단위\n",
    "- `feature` : 각 샘플에서 $y$를 결정하게 하는 각각의 독립 변수 $x$\n",
    "\n",
    "앞선 훈련 데이터에서 샘플의 수는 총 5개, 특성은 3개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따라서 독립 변수 $x$의 수가 (샘플의 수 * 특성의 수) = 15 개임을 알 수 있음\n",
    "\n",
    "이를 (샘플의 수 * 특성의 수) 크기의 행렬로 변환 가능하고 이를 $X$ 라 하면 다음과 같음\n",
    "\n",
    "$ \\begin{pmatrix} x_{11}&&x_{12}&&x_{13}\\\\x_{21}&&x_{22}&&x_{23}\\\\x_{31}&&x_{32}&&x_{33}\\\\x_{41}&&x_{42}&&x_{43}\\\\x_{51}&&x_{52}&&x_{53} \\end{pmatrix} $\n",
    "\n",
    "가중치 벡터 $W$를 곱하면 다음과 같음\n",
    "\n",
    "$ \n",
    "H(X) = XW =\n",
    "\\begin{pmatrix} x_{11}&&x_{12}&&x_{13} \\\\ x_{21}&&x_{22}&&x_{23} \\\\ x_{31}&&x_{32}&&x_{33} \\\\ x_{41}&&x_{42}&&x_{43} \\\\ x_{51}&&x_{52}&&x_{53} \\end{pmatrix} \n",
    "\\begin{pmatrix} w_1\\\\w_2\\\\w_3 \\end{pmatrix} =\n",
    "\\begin{pmatrix} \n",
    "    x_{11}w_1 + x_{12}w_2 + x_{13}w_3 \\\\ \n",
    "    x_{21}w_1 + x_{22}w_2 + x_{23}w_3 \\\\ \n",
    "    x_{31}w_1 + x_{32}w_2 + x_{33}w_3 \\\\ \n",
    "    x_{41}w_1 + x_{42}w_2 + x_{43}w_3 \\\\ \n",
    "    x_{51}w_1 + x_{52}w_2 + x_{53}w_3 \n",
    "\\end{pmatrix}\n",
    "$\n",
    "\n",
    "여기에 각 샘플에 더해지는 편향 $b$를 샘플 수만큼 차원을 가지는 편향 벡터 $B$를 만들어 더하면 다음과 같음\n",
    "\n",
    "$ \n",
    "H(X) = XW + B =\n",
    "\\begin{pmatrix} x_{11}&&x_{12}&&x_{13} \\\\ x_{21}&&x_{22}&&x_{23} \\\\ x_{31}&&x_{32}&&x_{33} \\\\ x_{41}&&x_{42}&&x_{43} \\\\ x_{51}&&x_{52}&&x_{53} \\end{pmatrix} \n",
    "\\begin{pmatrix} w_1\\\\w_2\\\\w_3 \\end{pmatrix} +\n",
    "\\begin{pmatrix} b\\\\b\\\\b\\\\b\\\\b \\end{pmatrix} =\n",
    "\\begin{pmatrix} \n",
    "    x_{11}w_1 + x_{12}w_2 + x_{13}w_3 + b \\\\ \n",
    "    x_{21}w_1 + x_{22}w_2 + x_{23}w_3 + b \\\\ \n",
    "    x_{31}w_1 + x_{32}w_2 + x_{33}w_3 + b \\\\ \n",
    "    x_{41}w_1 + x_{42}w_2 + x_{43}w_3 + b \\\\ \n",
    "    x_{51}w_1 + x_{52}w_2 + x_{53}w_3 + b\n",
    "\\end{pmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 행렬 연산을 이용한 PyTorch 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "#                               x1   x2   x3\n",
    "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
    "                               [93,  88,  93], \n",
    "                               [89,  91,  80], \n",
    "                               [96,  98,  100],   \n",
    "                               [73,  66,  70]])  \n",
    "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치와 편향 선언\n",
    "W = torch.zeros((3, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000             hypothesis: tensor([0., 0., 0., 0., 0.])             Cost: 29661.800781\n",
      "Epoch  100/1000             hypothesis: tensor([154.0433, 185.0925, 175.8312, 198.5701, 141.2221])             Cost: 5.754573\n",
      "Epoch  200/1000             hypothesis: tensor([154.0278, 185.0649, 175.9335, 198.5128, 141.2284])             Cost: 5.512386\n",
      "Epoch  300/1000             hypothesis: tensor([154.0120, 185.0385, 176.0329, 198.4569, 141.2353])             Cost: 5.281667\n",
      "Epoch  400/1000             hypothesis: tensor([153.9960, 185.0133, 176.1295, 198.4022, 141.2426])             Cost: 5.061868\n",
      "Epoch  500/1000             hypothesis: tensor([153.9797, 184.9892, 176.2233, 198.3488, 141.2504])             Cost: 4.852424\n",
      "Epoch  600/1000             hypothesis: tensor([153.9632, 184.9662, 176.3143, 198.2966, 141.2586])             Cost: 4.652705\n",
      "Epoch  700/1000             hypothesis: tensor([153.9465, 184.9442, 176.4028, 198.2456, 141.2672])             Cost: 4.462287\n",
      "Epoch  800/1000             hypothesis: tensor([153.9296, 184.9232, 176.4888, 198.1958, 141.2762])             Cost: 4.280604\n",
      "Epoch  900/1000             hypothesis: tensor([153.9126, 184.9032, 176.5724, 198.1471, 141.2855])             Cost: 4.107294\n",
      "Epoch 1000/1000             hypothesis: tensor([153.8955, 184.8841, 176.6536, 198.0995, 141.2951])             Cost: 3.941866\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    # 편향 b는 브로드 캐스팅되어 각 샘플에 더해집니다.\n",
    "    hypothesis = x_train.matmul(W) + b\n",
    "\n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch:4d}/{nb_epochs} \\\n",
    "            hypothesis: {hypothesis.squeeze().detach()} \\\n",
    "            Cost: {cost.item():.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
