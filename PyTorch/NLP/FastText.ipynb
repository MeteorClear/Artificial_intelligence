{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 패스트텍스트(FastText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec 이후에 나온 페이스북에서 개발한 방식\n",
    "\n",
    "메커니즘 자체는 Word2Vec의 확장\n",
    "\n",
    "Word2Vec와 FastText와의 가장 큰 차이점은\n",
    "\n",
    "Word2Vec는 단어를 쪼개질 수 없는 단위로 생각\n",
    "\n",
    "FastText는 하나의 단어 안에도 여러 단어들이 존재하는 것으로 간주, 서브워드(subword)를 고려하여 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 내부 단어(subword)의 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastText에서는 각 단어는 글자 단위 n-gram의 구성으로 취급\n",
    "\n",
    "n을 몇으로 결정하는지에 따라서 단어들이 얼마나 분리되는지 결정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예를 들어, n을 3으로 잡은 트라이그램(tri-gram)의 경우, \n",
    "\n",
    "apple은 시작과 끝을 의미하는 \\<, \\>를 도입하여, \\<ap, app, ppl, ple, le\\> 5개 내부 단어(subword) 토큰을 벡터로 변환\n",
    "\n",
    "추가적으로 하나를 더 벡터화하는데, 기존 단어에 \\<, 와 \\>를 붙인 토큰, \\<apple\\> 를 벡터화\n",
    "\n",
    "즉 n = 3인 경우, FastText는 단어 apple에 대해서 다음의 6개의 토큰을 벡터화\n",
    "\n",
    "\\<ap, app, ppl, ple, le\\>, \\<apple\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제 사용할 때는 n의 최소값과 최대값으로 범위를 설정 가능, 기본값으로는 각각 3과 6으로 설정\n",
    "\n",
    "n = 3 ~ 6인 경우 apple의 내부 단어는 다음과 같음\n",
    "\n",
    "\\<ap, app, ppl, ppl, le\\>, \\<app, appl, pple, ple\\>, \\<appl, pple\\>, ..., \\<apple\\>\n",
    "\n",
    "여기서 내부 단어들을 벡터화한다는 의미는 저 단어들에 대해서 Word2Vec을 수행한다는 의미\n",
    "\n",
    "내부 단어들의 벡터값을 얻었다면, 단어 apple의 벡터값은 저 위 벡터값들의 총 합으로 구성\n",
    "\n",
    "apple = \\<ap + app + ppl + ppl + le\\> + \\<app + appl + pple + ple\\> + \\<appl + pple\\> + , ..., + \\<apple\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모르는 단어(Out Of Vocabulary, OOV)에 대한 대응"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastText의 인공 신경망을 학습한 후에는 데이터 셋의 모든 단어의 각 n-gram에 대해서 워드 임베딩 됨\n",
    "\n",
    "데이터 셋만 충분한다면 위와 같은 내부 단어(Subword)를 통해 모르는 단어(Out Of Vocabulary, OOV)에 대해서도 다른 단어와의 유사도를 계산 가능\n",
    "\n",
    "예를 들어, FastText에서 birthplace(출생지)란 단어를 학습하지 않은 상태라 가정하면 \n",
    "\n",
    "다른 단어에서 birth와 place라는 내부 단어가 있었다면 FastText는 birthplace의 벡터를 얻을 수 있음\n",
    "\n",
    "이는 모르는 단어에 제대로 대처할 수 없는 Word2Vec, GloVe와는 다른 점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어 집합 내 빈도 수가 적었던 단어(Rare Word)에 대한 대응"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec의 경우에는 등장 빈도 수가 적은 단어(rare word)에 대해서는 임베딩의 정확도가 높지 않다는 단점이 있음\n",
    "\n",
    "참고할 수 있는 경우의 수가 적다보니 정확하게 임베딩이 되지 않는 경우 임\n",
    "\n",
    "FastText의 경우, 만약 단어가 희귀 단어라도, 그 단어의 n-gram이 다른 단어의 n-gram과 겹치는 경우라면, \n",
    "\n",
    "Word2Vec과 비교하여 비교적 높은 임베딩 벡터값을 얻을 수 있음\n",
    "\n",
    "이때문에 FastText가 노이즈가 많은 코퍼스에서 강점을 가짐\n",
    "\n",
    "모든 훈련 코퍼스에 오타(Typo)나 맞춤법이 틀린 단어가 없으면 이상적이겠지만, 실제 많은 비정형 데이터에는 오타가 섞여있음\n",
    "\n",
    "오타가 섞인 단어는 당연히 등장 빈도수가 매우 적으므로 일종의 희귀 단어가 됨\n",
    "\n",
    "Word2Vec에서는 오타가 섞인 단어는 임베딩이 제대로 되지 않지만 FastText는 이에 대해서도 일정 수준의 성능을 보임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec, FastText 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from lxml import etree\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetXML = open('ted_en-20160408.xml', 'r', encoding='UTF8')\n",
    "target_text = etree.parse(targetXML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are two reasons companies fail: they only do more of the same, or they only do what's new.\n",
      "To m\n"
     ]
    }
   ],
   "source": [
    "# xml 파일로부터 <content>와 </content> 사이의 내용만 가져온다.\n",
    "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
    "print(parse_text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are two reasons companies fail: they only do more of the same, or they only do what's new.\n",
      "To m\n"
     ]
    }
   ],
   "source": [
    "# 정규 표현식의 sub 모듈을 통해 content 중간에 등장하는 (Audio), (Laughter) 등의 배경음 부분을 제거.\n",
    "# 해당 코드는 괄호로 구성된 내용을 제거.\n",
    "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
    "print(content_text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Here are two reasons companies fail: they only do more of the same, or they only do what's new.\", 'To me the real, real solution to quality growth is figuring out the balance between two activities: exploration and exploitation.', 'Both are necessary, but it can be too much of a good thing.']\n",
      "273424\n"
     ]
    }
   ],
   "source": [
    "# 입력 코퍼스에 대해서 NLTK를 이용하여 문장 토큰화를 수행.\n",
    "sent_text = sent_tokenize(content_text)\n",
    "print(sent_text[:3])\n",
    "print(len(sent_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['here are two reasons companies fail they only do more of the same or they only do what s new ', 'to me the real real solution to quality growth is figuring out the balance between two activities exploration and exploitation ', 'both are necessary but it can be too much of a good thing ']\n",
      "273424\n"
     ]
    }
   ],
   "source": [
    "# 각 문장에 대해서 구두점을 제거하고, 대문자를 소문자로 변환.\n",
    "normalized_text = []\n",
    "for string in sent_text:\n",
    "    tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
    "    normalized_text.append(tokens)\n",
    "\n",
    "print(normalized_text[:3])\n",
    "print(len(normalized_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new'], ['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation'], ['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing']]\n",
      "273424\n"
     ]
    }
   ],
   "source": [
    "# 각 문장에 대해서 NLTK를 이용하여 단어 토큰화를 수행.\n",
    "result = [word_tokenize(sentence) for sentence in normalized_text]\n",
    "print(result[:3])\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec 학습\n",
    "model_wv = Word2Vec(sentences=result, vector_size=100, window=5, min_count=5, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastText 학습\n",
    "model_ft = FastText(sentences=result, vector_size=100, window=5, min_count=5, workers=4, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Key 'electrofishing' not present in vocabulary\"\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec는 모르는 단어에 대해서는 임베딩 벡터가 존재하지 않음\n",
    "try:\n",
    "    model_result = model_wv.wv.most_similar(\"electrofishing\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('electrolyte', 0.8746446967124939), ('electrolux', 0.8699601292610168), ('electro', 0.8579500913619995), ('electroencephalogram', 0.8560504913330078), ('electroshock', 0.8535223007202148), ('electrogram', 0.8337435722351074), ('electrons', 0.8243095874786377), ('electric', 0.8234108686447144), ('electrochemical', 0.822003185749054), ('electron', 0.8217300176620483)]\n"
     ]
    }
   ],
   "source": [
    "# FastText는 모르는 단어에 대해서 유사한 단어를 계산해서 출력 가능\n",
    "try:\n",
    "    model_result = model_ft.wv.most_similar(\"electrofishing\")\n",
    "    print(model_result)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wv.wv.save_word2vec_format('eng_w2v')\n",
    "model_ft.save('eng_ft')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한국어에서의 FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한국어의 경우에도 OOV 문제를 해결하기 위해 FastText를 적용하고자 하는 시도들이 있었음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 음절 단위"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "음절 단위의 임베딩의 경우에 n=3일때 '자연어처리'라는 단어에 대해 n-gram을 만들어보면 다음과 같음\n",
    "\n",
    "\\<자연, 자연어, 연어처, 어처리, 처리\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 자모 단위"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "더 나아가 자모 단위(초성, 중성, 종성 단위)로 임베딩하는 시도 또한 있었음\n",
    "\n",
    "음절 단위가 아니라, 자모 단위로 가게 되면 오타나 노이즈 측면에서 더 강한 임베딩을 기대해볼 수 있음\n",
    "\n",
    "'자연어처리'라는 단어에 대해서 초성, 중성, 종성을 분리하고, 만약, 종성이 존재하지 않는다면 ‘_’라는 토큰을 사용한다고 가정한다면 아래와 같이 분리가 가능\n",
    "\n",
    "ㅈ ㅏ _ ㅇ ㅕ ㄴ ㅇ ㅓ _ ㅊ ㅓ _ ㄹ ㅣ _\n",
    "\n",
    "분리된 결과에 대해서 n=3일 때, n-gram을 적용하여, 임베딩을 한다면 다음과 같음\n",
    "\n",
    "\\< ㅈ ㅏ, ㅈ ㅏ _, ㅏ _ ㅇ, ... \\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jamo import h2j, j2hcj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시 텍스트 데이터\n",
    "texts = [\n",
    "    \"안녕하세요\",\n",
    "    \"반갑습니다\",\n",
    "    \"자연어 처리를 배우고 있습니다\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 데이터를 자모 단위로 분리하는 함수\n",
    "def text_to_jamo(text):\n",
    "    return ' '.join([j for char in text for j in h2j(char)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ᄋ ᅡ ᆫ ᄂ ᅧ ᆼ ᄒ ᅡ ᄉ ᅦ ᄋ ᅭ', 'ᄇ ᅡ ᆫ ᄀ ᅡ ᆸ ᄉ ᅳ ᆸ ᄂ ᅵ ᄃ ᅡ', 'ᄌ ᅡ ᄋ ᅧ ᆫ ᄋ ᅥ   ᄎ ᅥ ᄅ ᅵ ᄅ ᅳ ᆯ   ᄇ ᅢ ᄋ ᅮ ᄀ ᅩ   ᄋ ᅵ ᆻ ᄉ ᅳ ᆸ ᄂ ᅵ ᄃ ᅡ']\n"
     ]
    }
   ],
   "source": [
    "# 자모 단위로 분리된 텍스트 데이터 생성\n",
    "jamo_texts = [text_to_jamo(text) for text in texts]\n",
    "print(jamo_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ᄋ', 'ᅡ', 'ᆫ', 'ᄂ', 'ᅧ', 'ᆼ', 'ᄒ', 'ᅡ', 'ᄉ', 'ᅦ', 'ᄋ', 'ᅭ'], ['ᄇ', 'ᅡ', 'ᆫ', 'ᄀ', 'ᅡ', 'ᆸ', 'ᄉ', 'ᅳ', 'ᆸ', 'ᄂ', 'ᅵ', 'ᄃ', 'ᅡ'], ['ᄌ', 'ᅡ', 'ᄋ', 'ᅧ', 'ᆫ', 'ᄋ', 'ᅥ', 'ᄎ', 'ᅥ', 'ᄅ', 'ᅵ', 'ᄅ', 'ᅳ', 'ᆯ', 'ᄇ', 'ᅢ', 'ᄋ', 'ᅮ', 'ᄀ', 'ᅩ', 'ᄋ', 'ᅵ', 'ᆻ', 'ᄉ', 'ᅳ', 'ᆸ', 'ᄂ', 'ᅵ', 'ᄃ', 'ᅡ']]\n"
     ]
    }
   ],
   "source": [
    "# 자모 단위로 분리된 텍스트 데이터를 토큰화\n",
    "tokenized_texts = [text.split() for text in jamo_texts]\n",
    "print(tokenized_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastText 모델 학습\n",
    "model = FastText(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, workers=4, sg=1, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00222797  0.00187597  0.00436019 ...  0.00302328 -0.00807463\n",
      "   0.00667974]\n",
      " [ 0.00229831 -0.00284881 -0.00224704 ... -0.00838212 -0.00152303\n",
      "   0.00330817]\n",
      " [-0.00710352  0.00064455 -0.00070119 ... -0.00264836 -0.00485959\n",
      "  -0.00391405]\n",
      " ...\n",
      " [ 0.00764866 -0.00201815  0.00386967 ...  0.00026743  0.007372\n",
      "   0.00338729]\n",
      " [-0.00222797  0.00187597  0.00436019 ...  0.00302328 -0.00807463\n",
      "   0.00667974]\n",
      " [-0.00122434  0.00388527  0.00217325 ...  0.00559431  0.00189277\n",
      "   0.00285902]]\n"
     ]
    }
   ],
   "source": [
    "# \"안녕하세요\"의 자모 단위 벡터 확인\n",
    "jamo_vector = model.wv[text_to_jamo(\"안녕하세요\").split()]\n",
    "print(jamo_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "model.save(\"fasttext_jamo.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 로드\n",
    "model = FastText.load(\"fasttext_jamo.model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
