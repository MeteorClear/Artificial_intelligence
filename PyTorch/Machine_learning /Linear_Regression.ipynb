{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x29ef99a92f0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 재실행해도 다음에도 같은 결과가 나오도록 랜덤 시드(random seed)를 수동으로 설정\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression(선형 회귀)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `training dataset` : 예측을 위해 사용하는 데이터(학습용 데이터)\n",
    "- `test dataset` : 모델이 얼마나 잘 작동하는지 판별하는 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PyTorch`는 `training dataset`이 `torch.tensor` 형태를 가지고 있어야 함\n",
    "\n",
    "입력과 출력을 각기 다른 텐서에 저장할 필요가 있음\n",
    "\n",
    "보편적으로 입력은 `x`, 출력은 `y`를 사용하여 표기함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$ X_{train} = \\begin{pmatrix} 1\\\\2\\\\3 \\end{pmatrix} $\n",
    "$ Y_{trian} = \\begin{pmatrix} 2\\\\4\\\\6 \\end{pmatrix} $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수 선언\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x_train)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.],\n",
      "        [4.],\n",
      "        [6.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(y_train)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 가설(Hypothesis) 수립"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Hypothesis(가설)` : `Machine Learning`에서 식을 세울때 세운 식, 임의로 추측하여 세우거나, 경험적으로 알고 있는 식일 수 있음 가설이 아니라고 판단되면 계속 수정해 나가게 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "선형 회귀의 가설(직선의 방정식)\n",
    "\n",
    "$ y = Wx + b $\n",
    "\n",
    "가설의 $H$를 따서 $y$대신 사용하기도 함\n",
    "\n",
    "$ H(x) = Wx + b $\n",
    "\n",
    "$W$ 를 `Weight(가중치)`, $b$ 를 `bias(편향)` 이라 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n",
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 가중치 W를 0으로 초기화하고 학습을 통해 값이 변경되는 변수임을 명시함.\n",
    "W = torch.zeros(1, requires_grad=True) \n",
    "# 가중치 W를 출력\n",
    "print(W)\n",
    "\n",
    "# 편향 b도 0으로 초기화하고, 학습을 통해 값이 변경되는 변수임을 명시\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "print(b)\n",
    "\n",
    "# W,b 둘다 0이므로 현 방정식은 y = 0 * x + 0\n",
    "# requires_grad 속성을 True로 설정하면 자동 미분 기능이 적용\n",
    "# 적용된 텐서에 연산을 하면, 계산 그래프가 생성되며 backward 함수를 호출하면 그래프로부터 자동으로 미분이 계산됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 직선의 방정식에 해당되는 가설을 선언\n",
    "hypothesis = x_train * W + b\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 비용 함수(Cost function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`비용 함수(cost function)` = `손실 함수(loss function)` = `오차 함수(error function)` = `목적 함수(objective function)`\n",
    "\n",
    "이 중 비용 함수(cost function) 와 손실 함수(loss function) 를 가장 많이 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Linear Regression(선형 회귀)`는 위와 같이 직선의 방정식을 지님\n",
    "\n",
    "데이터를 가장 잘 나타내는 직선의 방적식을 찾는 방법임\n",
    "\n",
    "`오차(error)`가 가장 적은 직선이 데이터를 가장 잘 나타내는 직선이라 할 수 있음\n",
    "\n",
    "수식적으로 단순히 `오차 = 실제값 - 예측값`으로 정의하면 오차값이 음수가 나오는 경우가 발생함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`총 오차(total error)`를 구하기 위해, 오차를 그냥 전부 더하는 것이 아니라, 각 오차들을 제곱해준 뒤에 전부 더함.\n",
    "\n",
    "$ 1\\over n $ $ \\displaystyle\\sum_{ i=1 }^{ n }{ [y^{(i)} - H(x^{(i)})]^2 } $\n",
    "\n",
    "이를 `평균 제곱 오차(Mean Squared Error, MSE)` 라 함\n",
    "\n",
    "MSE는 회귀 문제에 적절한 $W$와 $b$를 찾기위해 최적화된 식임, 평균 제곱 오차의 값을 최소값으로 만드는 $W$와 $b$를 찾는 것이 가장 훈련 데이터를 잘 반영한 직선을 찾아내는 일이기 때문\n",
    "\n",
    "MSE를 $W$와 $b$에 의한 비용 함수(cost function)로 재정의하면 다음과 같음\n",
    "\n",
    "$ cost(W,b) = $ $ 1\\over n $ $ \\displaystyle\\sum_{ i=1 }^{ n }{ [y^{(i)} - H(x^{(i)})]^2 } $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18.6667, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 선형 회귀의 비용 함수에 해당되는 평균 제곱 오차를 선언\n",
    "cost = torch.mean((hypothesis - y_train) ** 2) \n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 옵티마이저(Optimizer) - 경사 하강법(Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`옵티마이저(Optimizer)` 알고리즘 : 최적화 알고리즘, 여기에선 비용 함수(cost Function)의 값을 최소로 하는 $W$와 $b$를 찾는 방법\n",
    "\n",
    "`학습(training)` : 옵티마이저 알고리즘을 통해 적절한 $W$와 $b$를 찾아내는 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ H(x) = Wx + b $\n",
    "\n",
    "$ cost(W,b) = $ $ 1\\over n $ $ \\displaystyle\\sum_{ i=1 }^{ n }{ [y^{(i)} - H(x^{(i)})]^2 } $\n",
    "\n",
    "다음 식에서 가중치 $W$는 기울기, 편향 $b$는 절편과 같음\n",
    "\n",
    "기울기가 지나치게 크면 실제값과 예측값의 오차가 커지고, 기울기가 지나치게 작아도 실제값과 예측값의 오차가 커짐, 편향 또한 지나치게 크거나 작으면 오차가 커짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W$ 와 $cost$ 관계를 x, y로 두고 그래프로 그리면 아래로 볼록한 2차 방정식 그래프를 가짐\n",
    "\n",
    "$W$ 가 무한대로 커지면 $cost$ 도 무한대로 커지고 $W$ 가 무한대로 작아져도 $cost$도 무한대로 커짐\n",
    "\n",
    "따라서 $cost$ 가 가장 작은 지점은 맨 아래의 볼록한 지점인 그래프에서 꼭지점의 $W$ 임\n",
    "\n",
    "이를 찾는 과정에 사용하는 것이 `경사 하강법(Gradient Descent)`\n",
    "\n",
    "경사 하강법은 미분을 하여 접선의 기울기가 0에 가까운 지점을 찾아나감, $cost$가 최소화가 되는 지점은 접선의 기울기가 0이 되는 지점이며, 또한 미분값이 0이 되는 지점이기 때문\n",
    "\n",
    "즉 경사 하강법은 Cost function을 미분하여 현재 $W$에서의 접선의 기울기를 구하고, 접선의 기울기가 낮은 방향으로 $W$의 값을 변경하는 작업을 반복하는 것에 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ F = \\frac{ \\partial cost(W) }{ \\partial W } $\n",
    "\n",
    "- 기울기 $F$가 음수일때: $W$값이 증가\n",
    "- 기울기 $F$가 양수일때: $W$값이 감소\n",
    "\n",
    "$ W := W - \\alpha \\frac{ \\partial }{ \\partial W }cost(W) $\n",
    "\n",
    "$\\alpha$는 `학습률(learning rate)`, $W$ 값을 변경할 때, 얼마나 크게 변경할지를 결정 또는 그래프의 한 점으로보고 접선의 기울기가 0일 때까지 경사를 따라 내려간다는 관점에서는 얼마나 큰 폭으로 이동할지를 결정\n",
    "\n",
    "학습률 $\\alpha$를 무작정 크게 한다고 $W$를 빠르게 찾는 것은 아님, 오히려 지나치게 높으면 $cost$값이 발산하는 상황이 발생\n",
    "\n",
    "반대로 학습률 $\\alpha$가 지나치게 낮으면 학습속도가 느려짐\n",
    "\n",
    "따라서 적절한 $\\alpha$를 찾아내는게 중요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가설, 비용 함수, 옵티마이저는 머신 러닝 분야에서 사용되는 포괄적 개념\n",
    "\n",
    "풀고자하는 각 문제에 따라 가설, 비용 함수, 옵티마이저는 전부 다를 수 있음\n",
    "\n",
    "선형 회귀에 가장 적합한 비용 함수는 평균 제곱 오차, 옵티마이저는 경사 하강법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경사 하강법을 구현합니다. 아래의 'SGD'는 Stochastic Gradient Descent 경사 하강법의 일종입니다. lr은 학습률(learning rate)를 의미합니다. 학습 대상인 W와 b가 SGD의 입력됨\n",
    "optimizer = optim.SGD([W, b], lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient를 0으로 초기화\n",
    "optimizer.zero_grad() \n",
    "# 비용 함수를 미분하여 gradient 계산\n",
    "cost.backward() \n",
    "# W와 b를 업데이트\n",
    "optimizer.step() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000               W: 0.353,               b: 0.151,               Cost: 14.770963\n",
      "Epoch  100/2000               W: 1.746,               b: 0.577,               Cost: 0.047939\n",
      "Epoch  200/2000               W: 1.801,               b: 0.453,               Cost: 0.029624\n",
      "Epoch  300/2000               W: 1.843,               b: 0.356,               Cost: 0.018306\n",
      "Epoch  400/2000               W: 1.877,               b: 0.280,               Cost: 0.011312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  500/2000               W: 1.903,               b: 0.220,               Cost: 0.006990\n",
      "Epoch  600/2000               W: 1.924,               b: 0.173,               Cost: 0.004319\n",
      "Epoch  700/2000               W: 1.940,               b: 0.136,               Cost: 0.002669\n",
      "Epoch  800/2000               W: 1.953,               b: 0.107,               Cost: 0.001649\n",
      "Epoch  900/2000               W: 1.963,               b: 0.084,               Cost: 0.001019\n",
      "Epoch 1000/2000               W: 1.971,               b: 0.066,               Cost: 0.000630\n",
      "Epoch 1100/2000               W: 1.977,               b: 0.052,               Cost: 0.000389\n",
      "Epoch 1200/2000               W: 1.982,               b: 0.041,               Cost: 0.000240\n",
      "Epoch 1300/2000               W: 1.986,               b: 0.032,               Cost: 0.000149\n",
      "Epoch 1400/2000               W: 1.989,               b: 0.025,               Cost: 0.000092\n",
      "Epoch 1500/2000               W: 1.991,               b: 0.020,               Cost: 0.000057\n",
      "Epoch 1600/2000               W: 1.993,               b: 0.016,               Cost: 0.000035\n",
      "Epoch 1700/2000               W: 1.995,               b: 0.012,               Cost: 0.000022\n",
      "Epoch 1800/2000               W: 1.996,               b: 0.010,               Cost: 0.000013\n",
      "Epoch 1900/2000               W: 1.997,               b: 0.008,               Cost: 0.000008\n",
      "Epoch 2000/2000               W: 1.997,               b: 0.006,               Cost: 0.000005\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 2000 # 경사하강법을 반복할 횟수\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    hypothesis = x_train * W + b\n",
    "\n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() # 파이토치는 미분을 통해 얻은 기울기를 이전에 계산된 기울기 값에 누적함, 미분값을 계속 0으로 초기화시켜줘야 됨\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch:4d}/{nb_epochs} \\\n",
    "              W: {W.item():.3f}, \\\n",
    "              b: {b.item():.3f}, \\\n",
    "              Cost: {cost.item():.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최종 훈련 결과를 보면 최적의 기울기 $W$는 2에 가깝고, $b$는 0에 가까움\n",
    "\n",
    "현재 훈련 데이터가 x_train은 [[1], [2], [3]]이고 y_train은 [[2], [4], [6]]인 것을 감안하면\n",
    "\n",
    "실제 정답은 $W$가 2이고, $b$가 0인 $H(x) = 2x$ 이므로 거의 정답에 가깝게 학습됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
