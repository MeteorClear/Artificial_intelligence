{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch**: 메인 네임스페이스, 텐서 및 수학 함수 포함\n",
    "- **torch.autograd**: 미분을 위한 함수 포함, 자동미분을 제어용 콘텍스트 매니저(`enable_grad`/`no_grad`) 및 미분 가능 함수 정의용 기반 클래스 `Function` 포함\n",
    "- **torch.nn**: 신경망을 위한 데이터 구조 및 레이어 정의, `RNN`, `LSTM` 등의 레이어, `ReLU` 등 활성화 함수, `MSELoss` 등 손실함수 등이 포함\n",
    "- **torch.optim**: `SGD` 및 파라미터 최적화 알고리즘 포함\n",
    "- **torch.utils.data**: `mini-batch` 용 유틸함수 포함\n",
    "- **torch.onnx**: `ONNX(Open Neural Network Exchange)`의 포맷(서로 다른 딥러닝 프레임워크간 모델 공유 포맷)으로 모델을 익스포트(export)할 때 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Scalar**: 차원이 없는 단일 값\n",
    "- **Vector**: 1차원으로 구성된 값\n",
    "- **Matrix**: 2차원으로 구성된 값, 행렬, 2차원 배열\n",
    "- **Tensor**: 3차원 이상으로 구성된 값, 1차원/2차원 행렬도 텐서라 표현하기도 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Shape Convention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **2D Tensor(Typical Simple Setting)**\n",
    "  \n",
    "  |t| = (batch size, dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **3D Tensor(Typical Computer Vision)**\n",
    "\n",
    "  |t| = (batch size, width, height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **3D Tensor(Typical Natural Language Processing)**\n",
    "\n",
    "  |t| = (batch size, length, dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor with Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2. 3. 4. 5. 6. 7. 8. 9.] 9.0 [1. 2. 3. 4.]\n",
      "rank 1\n",
      "shape (10,)\n"
     ]
    }
   ],
   "source": [
    "# 1D\n",
    "t1 = np.array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])\n",
    "print(t1, t1[-1], t1[1:5])\n",
    "print('rank', t1.ndim)\n",
    "print('shape', t1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2.  3.]\n",
      " [ 4.  5.  6.]\n",
      " [ 7.  8.  9.]\n",
      " [10. 11. 12.]]\n",
      "rank 2\n",
      "shape (4, 3)\n"
     ]
    }
   ],
   "source": [
    "# 2D\n",
    "t2 = np.array([[1., 2., 3.,], [4., 5., 6.], [7., 8., 9.], [10., 11., 12.]])\n",
    "print(t2)\n",
    "print('rank', t2.ndim)\n",
    "print('shape', t2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1.  2.]\n",
      "  [ 3.  4.]\n",
      "  [ 5.  6.]]\n",
      "\n",
      " [[ 7.  8.]\n",
      "  [ 9. 10.]\n",
      "  [11. 12.]]\n",
      "\n",
      " [[13. 14.]\n",
      "  [15. 16.]\n",
      "  [17. 18.]]\n",
      "\n",
      " [[19. 20.]\n",
      "  [21. 22.]\n",
      "  [23. 24.]]]\n",
      "rank 3\n",
      "shape (4, 3, 2)\n"
     ]
    }
   ],
   "source": [
    "# 3D\n",
    "t3 = np.array([[[1., 2.], [3., 4.], [5., 6.]], [[7., 8.], [9., 10.], [11., 12.]], [[13., 14.], [15., 16.], [17., 18.]], [[19., 20.], [21., 22.], [23., 24.]]])\n",
    "print(t3)\n",
    "print('rank', t3.ndim)\n",
    "print('shape', t3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]) tensor(9.) tensor([1., 2., 3., 4.])\n",
      "rank 1\n",
      "size torch.Size([10])\n",
      "shape torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# 1D\n",
    "t1 = torch.FloatTensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])\n",
    "print(t1, t1[-1], t1[1:5])\n",
    "print('rank', t1.dim())\n",
    "print('size', t1.size())\n",
    "print('shape', t1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.],\n",
      "        [ 7.,  8.,  9.],\n",
      "        [10., 11., 12.]])\n",
      "rank 2\n",
      "size torch.Size([4, 3])\n",
      "shape torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "# 2D\n",
    "t2 = torch.FloatTensor([[1., 2., 3.,], [4., 5., 6.], [7., 8., 9.], [10., 11., 12.]])\n",
    "print(t2)\n",
    "print('rank', t2.dim())\n",
    "print('size', t2.size())\n",
    "print('shape', t2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.,  4.,  7., 10.]) 1 torch.Size([4])\n",
      "tensor([ 2.,  5.,  8., 11.]) 1 torch.Size([4])\n",
      "tensor([ 3.,  6.,  9., 12.]) 1 torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "print(t2[:, 0], t2[:, 0].dim(), t2[:, 0].size())\n",
    "print(t2[:, 1], t2[:, 1].dim(), t2[:, 1].size())\n",
    "print(t2[:, 2], t2[:, 2].dim(), t2[:, 2].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.],\n",
      "        [ 4.,  5.],\n",
      "        [ 7.,  8.],\n",
      "        [10., 11.]]) 2 torch.Size([4, 2])\n",
      "tensor([[ 1.],\n",
      "        [ 4.],\n",
      "        [ 7.],\n",
      "        [10.]]) 2 torch.Size([4, 1])\n",
      "tensor([[ 2.,  3.],\n",
      "        [ 5.,  6.],\n",
      "        [ 8.,  9.],\n",
      "        [11., 12.]]) 2 torch.Size([4, 2])\n",
      "tensor([[ 3.],\n",
      "        [ 6.],\n",
      "        [ 9.],\n",
      "        [12.]]) 2 torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "print(t2[:, :-1], t2[:, :-1].dim(), t2[:, :-1].size())\n",
    "print(t2[:, :-2], t2[:, :-2].dim(), t2[:, :-2].size())\n",
    "print(t2[:, 1:], t2[:, 1:].dim(), t2[:, 1:].size())\n",
    "print(t2[:, 2:], t2[:, 2:].dim(), t2[:, 2:].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Broadcasting` 연산에 사용되는 Tensor를 연산이 가능하도록 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 5.]])\n"
     ]
    }
   ],
   "source": [
    "# Vector + Vector\n",
    "m1 = torch.FloatTensor([[3, 3]])\n",
    "m2 = torch.FloatTensor([[2, 2]])\n",
    "print(m1 + m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 5.]])\n"
     ]
    }
   ],
   "source": [
    "# Vector + Scalar\n",
    "m1 = torch.FloatTensor([[1, 2]])\n",
    "m2 = torch.FloatTensor([3]) # [3] -> [3, 3]\n",
    "print(m1 + m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 5.],\n",
      "        [5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "# 2 x 1 Vector + 1 x 2 Vector\n",
    "m1 = torch.FloatTensor([[1, 2]]) # [[1, 2]] -> [[1, 2], [1, 2]]\n",
    "m2 = torch.FloatTensor([[3], [4]]) # [[3], [4]] -> [[3, 3], [4, 4]]\n",
    "print(m1 + m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Multiplication / Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Matrix 1:  torch.Size([2, 2])\n",
      "Shape of Matrix 2:  torch.Size([2, 1])\n",
      "tensor([[ 5.],\n",
      "        [11.]])\n"
     ]
    }
   ],
   "source": [
    "# normal matrix multiplication\n",
    "m1 = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "m2 = torch.FloatTensor([[1], [2]])\n",
    "print('Shape of Matrix 1: ', m1.shape) # 2 x 2\n",
    "print('Shape of Matrix 2: ', m2.shape) # 2 x 1\n",
    "print(m1.matmul(m2)) # 2 x 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Matrix 1:  torch.Size([2, 2])\n",
      "Shape of Matrix 2:  torch.Size([2, 1])\n",
      "tensor([[1., 2.],\n",
      "        [6., 8.]])\n",
      "tensor([[1., 2.],\n",
      "        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "# element-wise multiplication\n",
    "# broadcasting -> matrix multiplication\n",
    "m1 = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "m2 = torch.FloatTensor([[1], [2]])\n",
    "print('Shape of Matrix 1: ', m1.shape) # 2 x 2\n",
    "print('Shape of Matrix 2: ', m2.shape) # 2 x 1 / [[1], [2]] -> [[1, 1], [2, 2]]\n",
    "print(m1 * m2) # 2 x 2\n",
    "print(m1.mul(m2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2.])\n",
      "tensor(1.5000)\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([1, 2])\n",
    "print(t)\n",
    "print(t.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor(2.5000)\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "print(t)\n",
    "print(t.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3.])\n"
     ]
    }
   ],
   "source": [
    "print(t.mean(dim=0))\n",
    "# t.mean(dim=0)은 입력에서 첫번째 차원을 제거, 즉 행이 제거, 1과 3의 평균을 구하고, 2와 4의 평균을 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.5000, 3.5000])\n"
     ]
    }
   ],
   "source": [
    "print(t.mean(dim=1))\n",
    "# 두번째 차원을 제거, 즉 열이 제거, 1과 2의 평균을 구하고, 3과 4의 평균을 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.5000, 3.5000])\n"
     ]
    }
   ],
   "source": [
    "print(t.mean(dim=-1))\n",
    "# 마지막 차원을 제거, 여기에서는 열의 차원을 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.)\n",
      "tensor([4., 6.])\n",
      "tensor([3., 7.])\n",
      "tensor([3., 7.])\n"
     ]
    }
   ],
   "source": [
    "print(t.sum()) # 단순히 원소 전체의 덧셈을 수행\n",
    "print(t.sum(dim=0)) # 행을 제거\n",
    "print(t.sum(dim=1)) # 열을 제거\n",
    "print(t.sum(dim=-1)) # 열을 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max / ArgMax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Max` 텐서 사이에서 최대값\n",
    "- `ArgMax` 최대값의 인덱스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n",
      "torch.return_types.max(\n",
      "values=tensor([3., 4.]),\n",
      "indices=tensor([1, 1]))\n"
     ]
    }
   ],
   "source": [
    "print(t.max())\n",
    "print(t.max(dim=0)) # 첫번째 차원을 제거, argmax도 함께 리턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max:  tensor([3., 4.])\n",
      "Argmax:  tensor([1, 1])\n"
     ]
    }
   ],
   "source": [
    "print('Max: ', t.max(dim=0)[0]) # 최댓값\n",
    "print('Argmax: ', t.max(dim=0)[1]) # 최댓값의 인덱스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.max(\n",
      "values=tensor([2., 4.]),\n",
      "indices=tensor([1, 1]))\n",
      "torch.return_types.max(\n",
      "values=tensor([2., 4.]),\n",
      "indices=tensor([1, 1]))\n"
     ]
    }
   ],
   "source": [
    "print(t.max(dim=1))\n",
    "print(t.max(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`View` 는 PyTorch에서 Numpy의 `Reshape` 와 같은 역할, 즉 텐서의 크기(Shape)를 변경해주는 역할\n",
    "\n",
    "- view는 변경 전과 변경 후의 텐서 안의 원소의 개수가 유지\n",
    "- view는 사이즈가 -1로 설정되면 다른 차원으로부터 해당 값을 유추"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.,  1.,  2.],\n",
      "         [ 3.,  4.,  5.]],\n",
      "\n",
      "        [[ 6.,  7.,  8.],\n",
      "         [ 9., 10., 11.]]])\n",
      "torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "t = np.array([[[0, 1, 2], [3, 4, 5]],\n",
    "              [[6, 7, 8], [9, 10, 11]]])\n",
    "ft = torch.FloatTensor(t)\n",
    "print(ft)\n",
    "print(ft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.],\n",
      "        [ 3.,  4.,  5.],\n",
      "        [ 6.,  7.,  8.],\n",
      "        [ 9., 10., 11.]])\n",
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "print(ft.view([-1, 3])) # ft라는 텐서를 (?, 3)의 크기로 변경\n",
    "print(ft.view([-1, 3]).shape)\n",
    "# -1은 첫번째 차원은 사용자가 잘 모르겠으니 파이토치에 맡기겠다는 의미, 3은 두번째 차원의 길이는 3을 가지도록 하라는 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.,  1.,  2.]],\n",
      "\n",
      "        [[ 3.,  4.,  5.]],\n",
      "\n",
      "        [[ 6.,  7.,  8.]],\n",
      "\n",
      "        [[ 9., 10., 11.]]])\n",
      "torch.Size([4, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "print(ft.view([-1, 1, 3]))\n",
    "print(ft.view([-1, 1, 3]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squeeze / Unsqueeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`View` 와 마찬가지로 텐서의 원소 수를 그대로 유지하면서 모양과 차원을 조절\n",
    "- `Squeeze` 차원이 1인 경우 제거\n",
    "- `Unsqueeze` 특정 위치에 1인 차원을 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "2\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "ft = torch.FloatTensor([[0], [1], [2]])\n",
    "print(ft)\n",
    "print(ft.dim())\n",
    "print(ft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2.])\n",
      "1\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "print(ft.squeeze())\n",
    "print(ft.squeeze().dim())\n",
    "print(ft.squeeze().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2.])\n",
      "1\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "ft = torch.Tensor([0, 1, 2])\n",
    "print(ft)\n",
    "print(ft.dim())\n",
    "print(ft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.]])\n",
      "2\n",
      "torch.Size([1, 3])\n",
      "\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "2\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(ft.unsqueeze(0)) # 인덱스가 0부터 시작하므로 0은 첫번째 차원을 의미한다.\n",
    "print(ft.unsqueeze(0).dim())\n",
    "print(ft.unsqueeze(0).shape)\n",
    "print()\n",
    "print(ft.unsqueeze(1))\n",
    "print(ft.unsqueeze(1).dim())\n",
    "print(ft.unsqueeze(1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.]])\n",
      "torch.Size([1, 3])\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "# View 로도 동일한 결과 가능\n",
    "print(ft.view(1, -1))\n",
    "print(ft.view(1, -1).shape)\n",
    "print(ft.view(-1, 1))\n",
    "print(ft.view(-1, 1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type Casting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data type | dtype\n",
    "|---|---|\n",
    "8-bit integer (unsigned) | `torch.uint8`\n",
    "8-bit integer | `torch.int8`\n",
    "16-bit integer | `torch.int16` or `torch.short`\n",
    "32-bit integer | `torch.int32` or `torch.int`\n",
    "64-bit integer | `torch.int64` or `torch.long`\n",
    "16-bit floating point | `torch.float16` or `torch.half`\n",
    "32-bit floating point | `torch.float32` or `torch.float`\n",
    "64-bit floating point | `torch.float64` or `torch.double`\n",
    "\n",
    "`Type Casting` 자료형을 변환하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4])\n",
      "tensor([1., 2., 3., 4.])\n"
     ]
    }
   ],
   "source": [
    "lt = torch.LongTensor([1, 2, 3, 4])\n",
    "print(lt)\n",
    "print(lt.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 0, 1], dtype=torch.uint8)\n",
      "tensor([1, 0, 0, 1])\n",
      "tensor([1., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "bt = torch.ByteTensor([True, False, False, True])\n",
    "print(bt)\n",
    "print(bt.long())\n",
    "print(bt.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 텐서를 `연결(concatenate)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "torch.Size([2, 2])\n",
      "tensor([[5., 6.],\n",
      "        [7., 8.]])\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "y = torch.FloatTensor([[5, 6], [7, 8]])\n",
    "print(x)\n",
    "print(x.shape)\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.],\n",
      "        [7., 8.]])\n",
      "torch.Size([4, 2])\n",
      "\n",
      "tensor([[1., 2., 5., 6.],\n",
      "        [3., 4., 7., 8.]])\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "print(torch.cat([x, y], dim=0)) # dim=0은 첫번째 차원을 늘리라는 의미\n",
    "print(torch.cat([x, y], dim=0).shape)\n",
    "print()\n",
    "print(torch.cat([x, y], dim=1))\n",
    "print(torch.cat([x, y], dim=1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Stacking` `연결(concatenate)`을 하는 또 다른 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([1, 4])\n",
    "y = torch.FloatTensor([2, 5])\n",
    "z = torch.FloatTensor([3, 6])\n",
    "print(x.dim())\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 4.],\n",
      "        [2., 5.],\n",
      "        [3., 6.]])\n",
      "tensor([[1., 4.],\n",
      "        [2., 5.],\n",
      "        [3., 6.]])\n",
      "2\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "print(torch.stack([x, y, z]))\n",
    "print(torch.cat([x.unsqueeze(0), y.unsqueeze(0), z.unsqueeze(0)], dim=0)) # 위와 동일한 연산\n",
    "print(torch.stack([x, y, z]).dim())\n",
    "print(torch.stack([x, y, z]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "2\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(torch.stack([x, y, z], dim=1))\n",
    "print(torch.stack([x, y, z], dim=1).dim())\n",
    "print(torch.stack([x, y, z], dim=1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ones_like / zeros_like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `ones_like` 동일한 크기(shape)지만 1으로만 값이 채워진 텐서를 생성\n",
    "- `zeros_like` 동일한 크기(shape)지만 0으로만 값이 채워진 텐서를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [2., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[0, 1, 2], [2, 1, 0]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.ones_like(x))\n",
    "print(torch.zeros_like(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-place Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "덮어쓰기 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "print(x.mul(2.)) # 곱하기 2를 수행한 결과를 출력\n",
    "print(x) # 기존의 값 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "print(x.mul_(2.))  # 곱하기 2를 수행한 결과를 변수 x에 값을 저장하면서 결과를 출력\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
