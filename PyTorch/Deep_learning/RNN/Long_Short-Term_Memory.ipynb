{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 장단기 메모리(Long Short-Term Memory, LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 단순한 형태의 RNN을 `Vanilla RNN` 또는 `Simple RNN` 이라 한다.\n",
    "\n",
    "LSTM은 이런 RNN의 한계를 극복하기 위해 나온 변형 중 하나이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 장기 의존성 문제(the problem of Long-Term Dependencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN은 출력 결과가 이전의 계산 결과에 의존한다. 그러나 RNN은 비교적 짧은 시퀀스(sequence)에 대해서만 효과를 보이는 단점이 있다\n",
    "\n",
    "RNN의 `시점(time step)이 길어질 수록 앞의 정보가 뒤로 충분히 전달되지 못하는 현상`이 발생하기 때문인데,\n",
    "\n",
    "이때문에 시점이 충분히 긴 상황에서는 앞 시점의 정보가 전체 정보에 대한 영향력은 거의 의미가 없을 수도 있다\n",
    "\n",
    "만약 시점의 앞 쪽에 가장 중요한 정보가 위치해 있고 RNN이 충분한 기억력을 가지고 있지 못하면 예측이 부정확 할 수 있다.\n",
    "\n",
    "이를 `장기 의존성 문제(the problem of Long-Term Dependencies)`라고 한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM 구조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](vaniila_rnn_and_different_lstm_ver2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM은 은닉층의 메모리 셀에 입력 게이트, 망각 게이트, 출력 게이트를 추가하여 불필요한 기억을 지우고, 기억해야할 것들을 저장함\n",
    "\n",
    "따라서 은닉 상태(hidden state)를 계산하는 식이 전통적인 RNN보다 조금 더 복잡해졌으며 셀 상태(cell state)라는 값을 추가\n",
    "\n",
    "RNN과 비교하여 긴 시퀀스의 입력을 처리하는데 탁월한 성능을 보임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "셀 상태는 $C$로 전 시점의 셀 상태 $C_{t-1}$가 다음 시점의 셀 상태 $C_t$를 구하기 위한 입력으로서 사용\n",
    "\n",
    "은닉 상태값과 셀 상태값을 구하기 위해서 새로 추가 된 3개의 게이트를 사용, 이 3개의 게이트에는 공통적으로 시그모이드 함수가 존재\n",
    "\n",
    "시그모이드 함수를 지나면 0과 1사이의 값이 나오게 되는데 이 값들을 가지고 게이트를 조절\n",
    "\n",
    "시그모이드 함수를 $\\sigma$, 하이퍼볼릭탄젠트 함수를 $tanh$\n",
    "\n",
    "$W_{xi}$ , $W_{xg}$ , $W_{xf}$ , $W_{xo}$ 는 $x_t$와 함께 각 게이트에서 사용되는 4개의 가중치\n",
    "\n",
    "$W_{hi}$ , $W_{hg}$ , $W_{hf}$ , $W_{ho}$ 는 $h_{t-1}$와 함께 각 게이트에서 사용되는 4개의 가중치\n",
    "\n",
    "$b_i$ , $b_g$ , $b_f$ , $b_o$ 는 각 게이트에서 사용되는 4개의 편향"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 입력 게이트\n",
    "\n",
    "![alt text](inputgate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ g_t = tanh(W_{xg} x_t + W_{hg} h_{t-1} + b_g) $\n",
    "\n",
    "$ i_t = \\sigma (W_{xi} x_t + W_{hg} h_{t-1} + b_g) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력 게이트는 현재 정보를 기억하기 위한 게이트\n",
    "\n",
    "$g_t$는 하이퍼볼릭탄젠트 함수를 지나 -1 ~ 1 사이의 값을 가지고, \n",
    "\n",
    "$i_t$는 시그모이드 함수를 지나 0 ~ 1 사이의 값을 지닌다\n",
    "\n",
    "$i_t$의 값이 클수록 현재 시점의 입력을 많이 반영한다는 의미\n",
    "\n",
    "입력 게이트는 현재 시점의 입력을 얼마나 반영할지를 결정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 삭제 게이트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](forgetgate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ f_t = \\sigma(W_{xf}x_t + W_{hf}h_{t-1} + b_f) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "삭제 게이트는 기억을 삭제하기 위한 게이트\n",
    "\n",
    "$f_t$는 시그모이드 함수를 지나 0 ~ 1 사이의 값이 나오고, 해당 값이 곧 삭제 과정을 거친 정보의 양\n",
    "\n",
    "0에 가까울수록 정보가 많이 삭제된 것이고 1에 가까울수록 정보를 온전히 기억한 것\n",
    "\n",
    "삭제 게이트는 이전 시점의 입력을 얼마나 반영할지를 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 셀 상태(장기 상태)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](cellstate2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ C_t = f_t \\circ C_{t-1} + i_t \\circ g_t $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "셀 상태 $C_t$는 LSTM에서 장기 상태라고도 부름\n",
    "\n",
    "입력 게이트에서 구한 $i_t$, $g_t$ 두 값을 원소별 곱(entrywise product)을 진행, 이 값이 이번에 선택된 기억할 값\n",
    "\n",
    "입력 게이트에서 선택된 기억을 삭제 게이트의 결과값과 더함, 이 값을 현재 시점 t의 셀 상태로, 다음 t+1 시점의 LSTM 셀로 넘겨짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "삭제 게이트의 출력값 $f_t$가 0이면 이전 시점의 셀 상태값 $C_{t-1}$ 또한 값이 0이 되면서\n",
    "\n",
    "입력 게이트의 결과만이 현재 시점의 셀 상태값 $C_t$를 결정함, 이는 삭제 게이트가 완전히 닫히고 입력 게이트를 연 상태를 의미\n",
    "\n",
    "반대로 입력 게이트의 $i_t$값이 0이면 현재 시점의 셀 상태값 $C_t$는 이전 시점의 셀 상태값 $C_{t-1}$의 값에 의존함\n",
    "\n",
    "이는 입력 게이트를 완전히 닫고 삭제 게이트만을 연 상태를 의미\n",
    "\n",
    "결과적으로 삭제 게이트는 이전 시점의 입력을 얼마나 반영할지를 의미, 입력 게이트는 현재 시점의 입력을 얼마나 반영할지를 결정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 출력 게이트와 은닉 상태(단기 상태)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](outputgateandhiddenstate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ o_t = \\sigma (W_{xo} x_t + W_{ho} h_{t-1} + b_o) $\n",
    "\n",
    "$ h_t = o_t \\circ tanh(c_t) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출력 게이트는 현재 시점 t의 x값과 이전 시점 t-1의 은닉 상태가 시그모이드 함수를 지난 값\n",
    "\n",
    "해당 값은 현재 시점 t의 은닉 상태를 결정\n",
    "\n",
    "은닉 상태를 단기 상태라고 하기도 함\n",
    "\n",
    "은닉 상태는 장기 상태의 값이 하이퍼볼릭탄젠트 함수를 지나 -1 ~ 1 사이의 값\n",
    "\n",
    "해당 값은 출력 게이트의 값과 연산되어, 값이 걸러지는 효과가 발생, 단기 상태의 값은 또한 출력층으로도 향함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch 에서 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN 셀에서 LSTM으로 바꿔주면 사용가능하다\n",
    "\n",
    "nn.RNN(input_dim, hidden_size, batch_fisrt=True)\n",
    "\n",
    "->\n",
    "\n",
    "nn.LSTM(input_dim, hidden_size, batch_fisrt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 10    # 입력의 크기\n",
    "hidden_dim = 100  # 은닉 상태 크기\n",
    "layer_dim = 1     # 은닉층 개수\n",
    "output_dim = 1    # 출력의 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM 모델 정의\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        # 은닉 상태 크기\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # 은닉층 개수\n",
    "        self.layer_dim = layer_dim\n",
    "        \n",
    "        # LSTM 사용\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        \n",
    "        # 출력층\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 은닉 상태 초기화\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # 셀 초기화\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "\n",
    "        return out\n",
    "    \n",
    "model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 10])\n",
      "tensor([[[ 1.2039,  0.1609, -0.0956,  0.5675, -0.8150,  0.0067,  0.5392,\n",
      "          -1.1186, -0.0790,  1.0845],\n",
      "         [ 1.5401, -1.3782, -0.3112, -0.7934,  0.1500, -0.0784, -0.2337,\n",
      "          -0.5998, -0.2178,  0.3963],\n",
      "         [ 1.4409,  0.3609, -1.3276, -0.8240,  0.0078,  0.7238,  0.7650,\n",
      "          -1.5663,  0.3778,  0.0276],\n",
      "         [-1.8175, -1.2501,  0.2030,  0.5001, -1.0371,  0.0255,  0.0535,\n",
      "          -1.3016, -1.9942, -0.9052],\n",
      "         [ 2.2444, -0.3012,  0.3523,  0.3779,  0.8881,  0.7441, -1.0162,\n",
      "          -0.1795,  0.4447, -1.2160]],\n",
      "\n",
      "        [[ 0.2241,  0.1242, -0.3550,  0.4491, -0.3476, -0.4995, -0.2421,\n",
      "           0.0030,  2.1632,  0.2112],\n",
      "         [-1.3865, -0.5824, -0.6144,  0.7574, -0.5591, -1.0751,  0.3298,\n",
      "          -0.0296, -1.8326, -0.3389],\n",
      "         [ 0.0079,  1.5170,  1.4620, -0.4900, -0.9398, -0.7445,  1.0012,\n",
      "          -0.1598, -0.1366,  0.6847],\n",
      "         [-0.2086,  0.8868,  0.1302,  0.8639,  1.4595,  0.0491, -0.7165,\n",
      "           0.0652, -0.0911,  0.5923],\n",
      "         [ 0.3420, -0.7804, -0.4075, -1.0571,  0.7790,  1.3398,  0.6296,\n",
      "           0.1276, -0.2055, -0.3585]],\n",
      "\n",
      "        [[ 2.1597,  0.6553, -0.7412, -1.5968,  1.2955, -1.4588,  0.6585,\n",
      "           1.0706,  0.5769, -0.7065],\n",
      "         [-0.9237, -0.8442,  0.9280, -0.1752, -0.9415, -2.8264, -0.4368,\n",
      "           2.0086, -0.6638,  0.7837],\n",
      "         [-0.1114,  1.4869,  0.6709, -0.7388,  1.0186,  0.5910, -0.1820,\n",
      "          -0.0269, -0.1881,  0.9252],\n",
      "         [ 0.7225, -0.5428, -0.2227, -0.1190,  0.4672, -0.7522,  0.0581,\n",
      "          -0.2201,  0.2779, -1.4502],\n",
      "         [-1.1260,  0.0536, -0.9249,  0.3624,  1.8700, -0.7513,  2.1515,\n",
      "          -0.0675, -1.0416, -0.5921]]])\n"
     ]
    }
   ],
   "source": [
    "# 예시 입력\n",
    "seq_dim = 5  # 시퀀스 길이\n",
    "batch_size = 3\n",
    "input_data = torch.randn(batch_size, seq_dim, input_dim)\n",
    "print(input_data.shape)\n",
    "print(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "output = model(input_data)\n",
    "print(output.shape)  # (batch_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
